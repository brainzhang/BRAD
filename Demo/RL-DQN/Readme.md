# **DQN å¼ºåŒ–å­¦ä¹ é¡¹ç›® README**

## **ğŸ“Œ é¡¹ç›®ç®€ä»‹**
æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº **æ·±åº¦ Q ç½‘ç»œï¼ˆDeep Q-Network, DQNï¼‰** çš„å¼ºåŒ–å­¦ä¹ é¡¹ç›®ï¼Œä¸»è¦ç”¨äº OpenAI Gym ä¸­çš„ `CartPole-v1` ä»»åŠ¡ã€‚DQN é€šè¿‡**ç¥ç»ç½‘ç»œé€¼è¿‘ Q å€¼**ï¼Œå¹¶ä½¿ç”¨**ç»éªŒå›æ”¾ï¼ˆReplay Bufferï¼‰**å’Œ**ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰**æå‡è®­ç»ƒç¨³å®šæ€§ã€‚

---

## **ğŸ“‚ ç›®å½•ç»“æ„**
```bash
RL-DQN/
â”‚â”€â”€ ğŸ“ Config/                # é…ç½®æ–‡ä»¶å­˜æ”¾ï¼ˆè¶…å‚æ•°ã€ç¯å¢ƒè®¾ç½®ç­‰ï¼‰
â”‚   â”œâ”€â”€ hyperparameters.yaml  # DQN è®­ç»ƒå‚æ•°
â”‚
â”‚â”€â”€ ğŸ“ drl_env/               # Python è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœä½ ä½¿ç”¨ `venv` åˆ›å»ºï¼‰
â”‚   â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ ...
â”‚
â”‚â”€â”€ ğŸ“ environments/          # ç¯å¢ƒå°è£…ï¼ˆGym Wrappersã€è‡ªå®šä¹‰ç¯å¢ƒï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cartpole_env.py       # é€‚ç”¨äº Gym çš„ CartPole-v1 åŒ…è£…å™¨
â”‚
â”‚â”€â”€ ğŸ“ models/                # DQN ç›¸å…³çš„ç¥ç»ç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dqn_model.py          # DQN ä¸»ç½‘ç»œ
â”‚   â”œâ”€â”€ pretrained_model.py   # é¢„è®­ç»ƒæ¨¡å‹ç®¡ç†
â”‚
â”‚â”€â”€ ğŸ“ utils/                 # å·¥å…·ç±»ï¼ˆæ—¥å¿—ã€æ•°æ®å¤„ç†ç­‰ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataloader.py         # æ•°æ®åŠ è½½ï¼ˆå¦‚æœæœ‰ç¦»çº¿æ•°æ®ï¼‰
â”‚   â”œâ”€â”€ logger.py             # è®­ç»ƒæ—¥å¿—è®°å½•
â”‚
â”‚â”€â”€ ğŸ“ memory/                # ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆReplay Bufferï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ replay_buffer.py      # ç»éªŒå›æ”¾å®ç°
â”‚
â”‚â”€â”€ ğŸ“ agents/                # å¼ºåŒ–å­¦ä¹  Agent é€»è¾‘
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dqn_agent.py          # DQN æ™ºèƒ½ä½“
â”‚
â”‚â”€â”€ ğŸ“ experiments/           # è®­ç»ƒå®éªŒï¼ˆä¸åŒçš„è®­ç»ƒæ–¹æ¡ˆï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_cartpole.py     # è®­ç»ƒ CartPole-v1
â”‚   â”œâ”€â”€ train_custom_env.py   # è®­ç»ƒè‡ªå®šä¹‰ç¯å¢ƒ
â”‚
â”‚â”€â”€ main.py                   # é¡¹ç›®ä¸»å…¥å£ï¼ˆå¯é€‰ï¼‰
â”‚â”€â”€ train.py                   # è®­ç»ƒå…¥å£
â”‚â”€â”€ test.py                    # æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
â”‚â”€â”€ requirements.txt            # ä¾èµ–åŒ…åˆ—è¡¨
â”‚â”€â”€ README.md                   # é¡¹ç›®æ–‡æ¡£
```

---

## **ğŸ”§ ä¾èµ–å®‰è£…**
### **1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ï¼‰**
```bash
python -m venv drl_env
source drl_env/bin/activate  # Mac/Linux
# æˆ–è€…åœ¨ Windows ä¸Š
# drl_env\Scripts\activate
```

### **2. å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

å¦‚æœä½ æ²¡æœ‰ `requirements.txt`ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰‹åŠ¨å®‰è£…æ‰€éœ€çš„ Python åº“ï¼š
```bash
pip install gym numpy torch matplotlib pyyaml
```

---

## **ğŸš€ è®­ç»ƒ DQN ä»£ç†**
### **1. è¿è¡Œè®­ç»ƒè„šæœ¬**
```bash
python train.py
```

### **2. è®­ç»ƒå‚æ•°ï¼ˆ`Config/hyperparameters.yaml`ï¼‰**
```yaml
training:
  total_episodes: 1000
  batch_size: 64
  gamma: 0.99
  lr: 0.001
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 0.995
  target_update: 10
  buffer_size: 10000

model:
  input_dim: 4
  hidden_dim: 128
  output_dim: 2

logging:
  log_dir: ./logs
  log_interval: 20
```

### **3. è®­ç»ƒæ—¥å¿—**
è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ ä¼šåœ¨ `./logs/` ç›®å½•ä¸‹æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ï¼Œæ–¹ä¾¿å¯è§†åŒ–è®­ç»ƒè¿›åº¦ã€‚

---

## **ğŸ›  ä»£ç è¯´æ˜**
### **1. è®­ç»ƒè„šæœ¬ `train.py`**
```python
import yaml
from agents.dqn_agent import DQNAgent
from environments.cartpole_env import CartPoleEnvWrapper

# è¯»å–è¶…å‚æ•°
with open("Config/hyperparameters.yaml", "r") as f:
    config = yaml.safe_load(f)

env = CartPoleEnvWrapper()
agent = DQNAgent(config)

for episode in range(config['training']['total_episodes']):
    state = env.reset()
    done = False
    total_reward = 0

    while not done:
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)
        agent.memory.append((state, action, reward, next_state, done))
        state = next_state
        total_reward += reward

    print(f"Episode {episode}: Total Reward = {total_reward}")
```

### **2. ç»éªŒå›æ”¾ `memory/replay_buffer.py`**
```python
import random
import numpy as np
from collections import deque

class ReplayBuffer:
    def __init__(self, buffer_size):
        self.memory = deque(maxlen=buffer_size)

    def add(self, transition):
        self.memory.append(transition)

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def size(self):
        return len(self.memory)
```

---

## **ğŸ§ª æµ‹è¯•æ™ºèƒ½ä½“**
### **1. è¿è¡Œæµ‹è¯•è„šæœ¬**
```bash
python test.py
```

### **2. è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“**
```python
import torch
from agents.dqn_agent import DQNAgent
from environments.cartpole_env import CartPoleEnvWrapper

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
env = CartPoleEnvWrapper()
agent = DQNAgent("Config/hyperparameters.yaml")
agent.policy_net.load_state_dict(torch.load("./models/dqn_trained.pth"))

state = env.reset()
done = False
while not done:
    action = agent.select_action(state)
    state, _, done, _ = env.step(action)
    env.env.render()
```

---

## **ğŸ“œ è´¡çŒ® & è®¸å¯**
æœ¬é¡¹ç›®å¼€æºï¼Œéµå¾ª **MIT License**ï¼Œæ¬¢è¿è´¡çŒ®ä»£ç ï¼

```bash
git clone https://github.com/your-repo/RL-DQN.git
cd RL-DQN
git checkout -b new-feature-branch
git commit -m "Add new feature"
git push origin new-feature-branch
```

**æ¬¢è¿æäº¤ Pull Request æˆ– Issue ğŸš€ï¼**

